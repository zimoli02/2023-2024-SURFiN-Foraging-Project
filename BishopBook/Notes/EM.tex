\documentclass{article}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{accents}

\title{Bishop Book: Expectation Maximization}
\author{Zimo Li}
\date{15 November 2023}

\begin{document}
\maketitle

\section{Background}
\subsection{Gaussian Mixture Model}
\par\noindent
The Gaussian mixture distribution can be written as a linear superposition
of Gaussians in the form
\[
p(\mathbf{x}) = \sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x}|\mu_k,\Sigma_k)
\]

\par\noindent
We introduce a binary random variable \(\mathbf{z}\) to attribute 
each observation into a Gaussian model \(\mathcal{N}(\mu_k,\Sigma_k)\).
\bigskip

\par\noindent
Hence, the values of \(\mathbf{z}\) should be 0s and a \(z_k=1\). 
The index of \(k\) depends on values of \(\pi_k\):
\[p(z_k=1)=\pi_k\]
where
\[0 \leq \pi_k \leq 1 \]
\[\sum_{k=1}^{K}\pi_k=1 \]
\bigskip

\par\noindent
The conditional distribution of x given a particular value for \(\mathbf{z}\)
can be written in the form
\[
    p(\mathbf{x}|\mathbf{z})
    =\prod_{k=1}^{K}
    \mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)^{z_k}
\]
Therefore, the observation \(\mathbf{x}\) is linked
to its latent state.
\bigskip

\par\noindent
The marginal distirbution of \(\mathbf{x}\) is
\begin{align*}
    p(\mathbf{x}) &=
    \sum_{\mathbf{z}}^{}p(\mathbf{z})p(\mathbf{x}|\mathbf{z})\\
    &= \sum_{\mathbf{z}}^{}\prod_{k=1}^{K}(\pi_k\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k))^{z_k}\\
    &=\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)\\
\end{align*}
which applies for every observed data point \(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\).
\bigskip

\par\noindent
For each \(\mathbf{z}_k\), we can use a function \(\gamma(z_k)\) to 
denote whether \(z_k=1\) under the observation of \(\mathbf{x}\)
\begin{align*}
    \gamma(z_k) = p(z_k=1|\mathbf{x})
    &= \frac{p(z_k=1)p(\mathbf{x}|z_k=1)}{p(\mathbf{x})}\\
    &= \frac{p(z_k=1)p(\mathbf{x}|z_k=1)}{\sum_{\mathbf{z}}^{}p(\mathbf{z})p(\mathbf{x}|\mathbf{z})}\\
    &= \frac{p(z_k=1)p(\mathbf{x}|z_k=1)}{\sum_{j=1}^{K}p(z_j=1)p(\mathbf{x}|z_j=1)}\\
    &= \frac{\pi_k\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_j\mathcal{N}(\mathbf{x}|\mu_j, \Sigma_j)}\\
\end{align*}
so that \(\pi_k\) is the prior probability of \(z_k=1\) and \(\gamma(z_k)\)
is the posterior probability once the \(\mathbf{x}\) is observed.
\bigskip

\par\noindent
For a set of observations \(\mathbf{X}\) where \(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\)
are all independent from each other, the log likelihood of observing
\(\mathbf{X}\) is
\[
\ln{p(\mathbf{X}|\mathbf{\pi},\mathbf{\mu},\mathbf{\Sigma})}
=\sum_{n=1}^{N}\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}
\]

\subsection{Likelihood}
\subsubsection{\(\mu_k\)}
\par\noindent
First, calculate the derivative of the log likelihood with respect to one of the means \(\mu_k\)
\begin{align*}
    \frac{d\ln{p(\mathbf{X}|\mathbf{\pi},\mathbf{\mu},\mathbf{\Sigma})}}{d\mu_k}
    &= \sum_{n=1}^{N} \frac{d\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}}{d\mu_k}\\
    &= \sum_{n=1}^{N} \frac{1}{\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}}\pi_k\frac{d\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)}{d\mu_k}\\
    &= \sum_{n=1}^{N} \frac{1}{\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}}\pi_k\{\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)\Sigma_k^{-1}(\mathbf{x}_n-\mu_k)\}\\
    &= \sum_{n=1}^{N} \frac{\pi_k\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)}{\sum_{j=1}^{K}\pi_j\mathcal{N}(\mathbf{x}|\mu_j, \Sigma_j)}\Sigma_k^{-1}(\mathbf{x}_n-\mu_k)\\
    &= \sum_{n=1}^{N} \gamma(z_{nk})\Sigma_k^{-1}(\mathbf{x}_n-\mu_k)
\end{align*}
\bigskip

\par\noindent
To maximize the log likelihood with respect to \(\mu_k\), we shall set this derivative to 0:
\begin{align*}
    \sum_{n=1}^{N} \gamma(z_{nk})\Sigma_k^{-1}(\mathbf{x}_n-\mu_k) &= 0
\end{align*}
Multiplying by \(\mathbf{\Sigma_k}^{-1}\) which is assumed to be nonsingular:
\begin{align*}
    \sum_{n=1}^{N} \gamma(z_{nk})\mathbf{x}_n &= \sum_{n=1}^{N} \gamma(z_{nk})\mu_k\\
    \mu_k &= \frac{1}{\sum_{n=1}^{N} \gamma(z_{nk})}\sum_{n=1}^{N} \gamma(z_{nk})\mathbf{x}_n\\
    &= \frac{1}{N_k}\sum_{n=1}^{N} \gamma(z_{nk})\mathbf{x}_n
\end{align*}
\bigskip

\subsubsection{\(\Sigma_k\)}
\par\noindent
Calculate the derivative of the log likelihood with respect to one of the means \(\Sigma_k\)
\begin{align*}
    \frac{d\ln{p(\mathbf{X}|\mathbf{\pi},\mathbf{\mu},\mathbf{\Sigma})}}{d\Sigma_k}
    &= \sum_{n=1}^{N} \frac{d\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}}{d\Sigma_k}\\
    &= \sum_{n=1}^{N} \frac{1}{\ln{\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\mathbf{x_n}|\mu_k, \Sigma_k)\}}}\pi_k\frac{d\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)}{d\Sigma_k}
\end{align*}
\bigskip


\subsubsection{\(\pi_k\)}


\section{EM Process}
\subsection{E}



\subsection{M}


\end{document}